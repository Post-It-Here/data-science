{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>4644c4</th>\n",
       "      <th>libertarian</th>\n",
       "      <th>news</th>\n",
       "      <th>1455650969</th>\n",
       "      <th>DrWinters</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>12728</th>\n",
       "      <th>1641</th>\n",
       "      <th>0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>464kvf</td>\n",
       "      <td>libertarian</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455657e+09</td>\n",
       "      <td>unknownman19</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50364.0</td>\n",
       "      <td>14436.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>466hv9</td>\n",
       "      <td>libertarian</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455683e+09</td>\n",
       "      <td>hp_chabanais</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>462wog</td>\n",
       "      <td>libertarian</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455636e+09</td>\n",
       "      <td>ghostofpennwast</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231424.0</td>\n",
       "      <td>80539.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disclaimer : i think obama should nominate som...</td>\n",
       "      <td>d028c5d</td>\n",
       "      <td>politics</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455651e+09</td>\n",
       "      <td>degausse</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>463sa9</td>\n",
       "      <td>politics</td>\n",
       "      <td>news</td>\n",
       "      <td>1.455647e+09</td>\n",
       "      <td>trash_reason</td>\n",
       "      <td>371.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5613.0</td>\n",
       "      <td>1361.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Unnamed: 0   4644c4  libertarian  \\\n",
       "0                                                NaN   464kvf  libertarian   \n",
       "1                                                NaN   466hv9  libertarian   \n",
       "2                                                NaN   462wog  libertarian   \n",
       "3  disclaimer : i think obama should nominate som...  d028c5d     politics   \n",
       "4                                                NaN   463sa9     politics   \n",
       "\n",
       "   news    1455650969        DrWinters      3    0     12728     1641  0.1  \n",
       "0  news  1.455657e+09     unknownman19   20.0  0.0   50364.0  14436.0  0.0  \n",
       "1  news  1.455683e+09     hp_chabanais    1.0  0.0     178.0      3.0  0.0  \n",
       "2  news  1.455636e+09  ghostofpennwast   18.0  0.0  231424.0  80539.0  1.0  \n",
       "3  news  1.455651e+09         degausse    3.0  0.0       1.0   1941.0  0.0  \n",
       "4  news  1.455647e+09     trash_reason  371.0  0.0    5613.0   1361.0  0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('comments.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2725999, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disclaimer : i think obama should nominate som...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>either way the process will be dragged out unt...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>republicans have always battled with severe ca...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>politics were so different back then. people o...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>equally ridiculous , here 's a 1970 law review...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>if you include the citizens conscripted with t...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&amp;gt ; \" more mistakes than achievements \" if t...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i am certain the guardian will issue a retract...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&amp;gt ; when the fishing gets tough , penguins s...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i 'm doing my part to keep those hard working ...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comments Subreddit\n",
       "3   disclaimer : i think obama should nominate som...      news\n",
       "5   either way the process will be dragged out unt...      news\n",
       "6   republicans have always battled with severe ca...      news\n",
       "7   politics were so different back then. people o...      news\n",
       "8   equally ridiculous , here 's a 1970 law review...      news\n",
       "9   if you include the citizens conscripted with t...      news\n",
       "11  &gt ; \" more mistakes than achievements \" if t...      news\n",
       "12  i am certain the guardian will issue a retract...      news\n",
       "13  &gt ; when the fishing gets tough , penguins s...      news\n",
       "15  i 'm doing my part to keep those hard working ...      news"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "df = df.rename(columns={'Unnamed: 0':'Comments',\n",
    "                          'news':'Subreddit'},\n",
    "               inplace= False)\n",
    "df = df[['Comments', 'Subreddit']]\n",
    "df = df.dropna()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2541739, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['news', 'lifestyle', 'learning', 'humor', 'entertainment',\n",
       "       'television', 'gaming'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Unique Subreddits\n",
    "df['Subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text\n",
    "#df[\"Comments\"] = df[\"Comments\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dataframe in-place and reset the index\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160478,) (381261,) (2160478,) (381261,)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X = df['Comments']\n",
    "y = df['Subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105789     great catch. i still say crawl space is the gr...\n",
       "2506978    http : //www.investors.com/politics/editorials...\n",
       "633407     more than 50 million years ago , canada 's arc...\n",
       "203286     i hate celebrity worship , what the fuck are w...\n",
       "1412126    the question some will want an answer to is , ...\n",
       "                                 ...                        \n",
       "110268     franklin roosevelt was responsable for the per...\n",
       "1692743                                           panthony !\n",
       "2356330                              yes ! ! ! thank you : )\n",
       "2229084    i do n't understand the removal of iron bars ,...\n",
       "2219110                                       anal probing ?\n",
       "Name: Comments, Length: 2160478, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline\n",
    "nb = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_Subreddit = df['Subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.853234398482929\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "entertainment       0.89      0.85      0.87     54012\n",
      "         news       0.82      0.92      0.86     82736\n",
      "       gaming       0.80      0.78      0.79     53338\n",
      "    lifestyle       0.91      0.78      0.84     37303\n",
      "     learning       0.83      0.88      0.85     54642\n",
      "   television       0.87      0.87      0.87     53799\n",
      "        humor       0.92      0.83      0.87     45431\n",
      "\n",
      "     accuracy                           0.85    381261\n",
      "    macro avg       0.86      0.84      0.85    381261\n",
      " weighted avg       0.86      0.85      0.85    381261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictions\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=My_Subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_predictions(post, answers=2):\n",
    "  \"\"\" takes a post and returns the top categories it fits in \"\"\"\n",
    "\n",
    "  # get the predicted probabilities for each class\n",
    "  preds = pd.Series(nb.predict_proba(post)[0])\n",
    "\n",
    "  # save each class to the Series index\n",
    "  preds.index = nb.classes_\n",
    "\n",
    "  # sort to get the most likely classes\n",
    "  preds = preds.sort_values(ascending=False)\n",
    "\n",
    "  # return the top num_answers results in dict format\n",
    "  return preds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test posts\n",
    "life_post =[ \"\"\" I live a very healthy life! I tend wake up early to eat breakfast\n",
    "                and I go a morning run \"\"\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lifestyle    0.413515\n",
       "humor        0.199555\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_predictions(education_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_post =[ \"\"\" That guy was extremely funny, I hope to be a comedian because im super funny and always\n",
    "                    tell the best jokes\"\"\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "humor            0.323000\n",
       "entertainment    0.224966\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_predictions(jokes_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifepost_ =[ \"\"\" To be honest we got to the “re-arrange furniture \n",
    "                to see how it looks” stage of quarantine a lot faster than I expected.\"\"\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lifestyle    0.409048\n",
       "gaming       0.319447\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_predictions(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pickle import dump\n",
    "# save the model\n",
    "filename = 'reddit_model.plk'\n",
    "pickle.dump(nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
