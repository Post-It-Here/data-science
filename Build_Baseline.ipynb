{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learnpython</td>\n",
       "      <td>Anyone care to share their resume that got the...</td>\n",
       "      <td>I feel a lot of self taught programmers who wa...</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fitness</td>\n",
       "      <td>Just benched 315 for the first time in over 20...</td>\n",
       "      <td>I'm 42, got back to the gym after my divorce b...</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tifu</td>\n",
       "      <td>TIFU by showing my grandmom that she had been ...</td>\n",
       "      <td>This did happen today. My grandmother is reall...</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relationships</td>\n",
       "      <td>[UPDATE] I [27M] think my fiancee [27F] might ...</td>\n",
       "      <td>Original post: https://iy.reddit.com/r/relatio...</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nosleep</td>\n",
       "      <td>I was almost involved in a school shooting</td>\n",
       "      <td>I’ve been wanting to get something off of my c...</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit                                              title  \\\n",
       "0    learnpython  Anyone care to share their resume that got the...   \n",
       "1        Fitness  Just benched 315 for the first time in over 20...   \n",
       "2           tifu  TIFU by showing my grandmom that she had been ...   \n",
       "3  relationships  [UPDATE] I [27M] think my fiancee [27F] might ...   \n",
       "4        nosleep         I was almost involved in a school shooting   \n",
       "\n",
       "                                           body_text  upvote_ratio  \n",
       "0  I feel a lot of self taught programmers who wa...          0.95  \n",
       "1  I'm 42, got back to the gym after my divorce b...          0.95  \n",
       "2  This did happen today. My grandmother is reall...          0.90  \n",
       "3  Original post: https://iy.reddit.com/r/relatio...          0.92  \n",
       "4  I’ve been wanting to get something off of my c...          0.96  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('reddit.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6888, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel a lot of self taught programmers who wa...</td>\n",
       "      <td>learnpython</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm 42, got back to the gym after my divorce b...</td>\n",
       "      <td>Fitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This did happen today. My grandmother is reall...</td>\n",
       "      <td>tifu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Original post: https://iy.reddit.com/r/relatio...</td>\n",
       "      <td>relationships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’ve been wanting to get something off of my c...</td>\n",
       "      <td>nosleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>So I am a doughy Iowan named James, and a few ...</td>\n",
       "      <td>IAmA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The man at the counter asked the older boy, \"S...</td>\n",
       "      <td>Jokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>On my seventh birthday, my mother made me a ra...</td>\n",
       "      <td>nosleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yesterday I got onto a subway train around rus...</td>\n",
       "      <td>tifu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A month ago I posted to [r/slavelabour](https:...</td>\n",
       "      <td>IAmA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text      Subreddit\n",
       "0  I feel a lot of self taught programmers who wa...    learnpython\n",
       "1  I'm 42, got back to the gym after my divorce b...        Fitness\n",
       "2  This did happen today. My grandmother is reall...           tifu\n",
       "3  Original post: https://iy.reddit.com/r/relatio...  relationships\n",
       "4  I’ve been wanting to get something off of my c...        nosleep\n",
       "5  So I am a doughy Iowan named James, and a few ...           IAmA\n",
       "6  The man at the counter asked the older boy, \"S...          Jokes\n",
       "7  On my seventh birthday, my mother made me a ra...        nosleep\n",
       "8  Yesterday I got onto a subway train around rus...           tifu\n",
       "9  A month ago I posted to [r/slavelabour](https:...           IAmA"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "df = df.rename(columns={'body_text':'Text',\n",
    "                          'subreddit':'Subreddit'},\n",
    "               inplace= False)\n",
    "df = df[['Text', 'Subreddit']]\n",
    "df = df.dropna()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6888, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['learnpython', 'Fitness', 'tifu', 'relationships', 'nosleep',\n",
       "       'IAmA', 'Jokes', 'dadjokes', 'relationship_advice',\n",
       "       'personalfinance'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Unique Subreddits\n",
    "df['Subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text\n",
    "#df[\"Comments\"] = df[\"Comments\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def  clean_text(df, text_field, new_text_field_name):\n",
    "    df[new_text_field_name] = df[text_field].str.lower()\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    \n",
    "    return df\n",
    "data_clean = clean_text(df, 'Text', 'text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/CassidyEllis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel a lot of self taught programmers who wa...</td>\n",
       "      <td>learnpython</td>\n",
       "      <td>feel lot self taught programmers want get job ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm 42, got back to the gym after my divorce b...</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>im got back gym divorce refuge teens early how...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This did happen today. My grandmother is reall...</td>\n",
       "      <td>tifu</td>\n",
       "      <td>happen today grandmother really superstitious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Original post: https://iy.reddit.com/r/relatio...</td>\n",
       "      <td>relationships</td>\n",
       "      <td>original post first wanted thank everybody com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’ve been wanting to get something off of my c...</td>\n",
       "      <td>nosleep</td>\n",
       "      <td>ive wanting get something chest long time pers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text      Subreddit  \\\n",
       "0  I feel a lot of self taught programmers who wa...    learnpython   \n",
       "1  I'm 42, got back to the gym after my divorce b...        Fitness   \n",
       "2  This did happen today. My grandmother is reall...           tifu   \n",
       "3  Original post: https://iy.reddit.com/r/relatio...  relationships   \n",
       "4  I’ve been wanting to get something off of my c...        nosleep   \n",
       "\n",
       "                                          text_clean  \n",
       "0  feel lot self taught programmers want get job ...  \n",
       "1  im got back gym divorce refuge teens early how...  \n",
       "2  happen today grandmother really superstitious ...  \n",
       "3  original post first wanted thank everybody com...  \n",
       "4  ive wanting get something chest long time pers...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feel lot self taught programmers want get job ...</td>\n",
       "      <td>learnpython</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im got back gym divorce refuge teens early how...</td>\n",
       "      <td>Fitness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happen today grandmother really superstitious ...</td>\n",
       "      <td>tifu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>original post first wanted thank everybody com...</td>\n",
       "      <td>relationships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ive wanting get something chest long time pers...</td>\n",
       "      <td>nosleep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean      Subreddit\n",
       "0  feel lot self taught programmers want get job ...    learnpython\n",
       "1  im got back gym divorce refuge teens early how...        Fitness\n",
       "2  happen today grandmother really superstitious ...           tifu\n",
       "3  original post first wanted thank everybody com...  relationships\n",
       "4  ive wanting get something chest long time pers...        nosleep"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_clean[['text_clean', 'Subreddit']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5854,) (1034,) (5854,) (1034,)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X = df['text_clean']\n",
    "y = df['Subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline\n",
    "nb = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_Subreddit = df['Subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6034816247582205\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        learnpython       0.92      0.86      0.89       117\n",
      "            Fitness       0.94      0.76      0.84        95\n",
      "               tifu       0.48      0.12      0.19        94\n",
      "      relationships       1.00      0.05      0.10        94\n",
      "            nosleep       0.98      0.82      0.89       112\n",
      "               IAmA       0.33      0.99      0.50       100\n",
      "              Jokes       0.82      0.82      0.82        97\n",
      "           dadjokes       0.49      0.75      0.59       111\n",
      "relationship_advice       0.52      0.63      0.57       108\n",
      "    personalfinance       0.48      0.12      0.20       106\n",
      "\n",
      "           accuracy                           0.60      1034\n",
      "          macro avg       0.69      0.59      0.56      1034\n",
      "       weighted avg       0.69      0.60      0.57      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictions\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=My_Subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_patt...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cl = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ('classifier', RandomForestClassifier()),\n",
    "              ])\n",
    "cl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7311411992263056\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        learnpython       0.97      0.83      0.89       117\n",
      "            Fitness       0.89      0.89      0.89        95\n",
      "               tifu       0.48      0.50      0.49        94\n",
      "      relationships       0.49      0.72      0.59        94\n",
      "            nosleep       0.92      0.88      0.90       112\n",
      "               IAmA       0.89      0.93      0.91       100\n",
      "              Jokes       0.81      0.78      0.80        97\n",
      "           dadjokes       0.54      0.45      0.49       111\n",
      "relationship_advice       0.57      0.62      0.60       108\n",
      "    personalfinance       0.82      0.71      0.76       106\n",
      "\n",
      "           accuracy                           0.73      1034\n",
      "          macro avg       0.74      0.73      0.73      1034\n",
      "       weighted avg       0.75      0.73      0.73      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictions\n",
    "y_pred = cl.predict(X_test)\n",
    "\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=My_Subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_predictions(post, answers=2):\n",
    "  \"\"\" takes a post and returns the top categories it fits in \"\"\"\n",
    "\n",
    "  # get the predicted probabilities for each class\n",
    "  preds = pd.Series(nb.predict_proba(post)[0])\n",
    "\n",
    "  # save each class to the Series index\n",
    "  preds.index = nb.classes_\n",
    "\n",
    "  # sort to get the most likely classes\n",
    "  preds = preds.sort_values(ascending=False)\n",
    "\n",
    "  # return the top num_answers results in dict format\n",
    "  return preds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test posts\n",
    "life_post =[ \"\"\" I live a very healthy life! I tend wake up early to eat breakfast\n",
    "                and I go a morning run \"\"\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fitness    0.274768\n",
       "nosleep    0.206433\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_predictions(life_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_post =[ \"\"\" That guy was extremely funny, I hope to be a comedian because im super funny and always\n",
    "                    tell the best jokes\"\"\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relationship_advice    0.232657\n",
       "tifu                   0.223674\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_predictions(jokes_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifepost_ =[ \"\"\" To be honest we got to the “re-arrange furniture \n",
    "                to see how it looks” stage of quarantine a lot faster than I expected.\"\"\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nosleep                0.185864\n",
       "relationship_advice    0.172604\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_predictions(lifepost_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pickle import dump\n",
    "# save the model\n",
    "filename = 'reddit_model.plk'\n",
    "pickle.dump(nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
